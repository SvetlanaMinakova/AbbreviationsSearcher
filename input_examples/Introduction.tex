\section{Introduction}
\label{sec:Intro}

Convolutional Neural Networks (CNNs) are biologically inspired graph computational models, characterized by high degree of available parallelism. Due to their ability to handle large, unstructured data, CNNs are widely used to perform various tasks in areas such as computer vision and natural language processing~\cite{Alom2018}. The CNNs execution typically includes two phases: training and  inference~\cite{Alom2018}. At the training phase the optimal CNN parameters are established. At the inference phase, a trained CNN is applied to the actual data and performs the task for which the CNN is designed. Due to the high complexity of state-of-the-art CNNs, their training and inference phases are usually performed by high-performance platforms, and provided as cloud services. However, some applications, e.g. ~\cite{MLInMedicineSecurity, MLInCars,MLinMedicine}, require high-throughput execution of the CNNs inference, which cannot be provided as a cloud service. These applications are typically deployed on embedded devices.

 
Many modern embedded devices are based on multi-processor systems-on-chip
(MPSoCs)~\cite{Martin:DAC06}: complex integrated circuits, that consist of processing elements with specific functionalities.
Due to their specific design, MPSoCs offer energy-efficient and high-performance
solutions for applications running on embedded devices. In addition
to hosting various processing elements, capable of running the CNN
inference, such as central processing units (CPUs), embedded graphics
processing units (embedded GPUs), and field-programmable gate arrays
(FPGAs), MPSoCs integrate many other components, such as communication network 
components and video accelerators, that allow to deploy the entire embedded application on a single
chip. Therefore, MPSoCs seem to be a promising solution for the deployment
of the CNN inference phase on embedded devices. 

However, achieving high-throughput execution of the computationally-intensive CNN inference phase on embedded CPUs-GPUs MPSoCs is a complex task. 

On the one hand, a high-throughput CNN inference execution requires effective utilization of the parallelism, available in a CNN. The parallelism available in a CNN can be divided into two different types: task-level (pipeline) and data-level parallelism. The parallelism, available among CNN layers, hereinafter referred as task-level parallelism~\cite{ReindersParallelism}, involves execution of several CNN layers in a parallel pipelined fashion, where each layer may perform computations, different from the computations,
performed by other CNN layers. Utilization of this type of parallelism allows to reduce the overall computation time, and increase the overall  CNN inference throughput, compared to  sequential execution of CNN layers~\cite{Wang2019HighThroughputCI}. The parallelism, available within a CNN layer, hereinafter referred as data-level parallelism~\cite{ReindersParallelism}, involves the same computation, e.g., Convolution, performed by a CNN layer over the CNN layer input data partitions. Utilization of this type of parallelism allows to improve the CNN inference throughput by accelerating the execution of  individual CNN layers~\cite{tensorflow2015, Caffe2, DLSurvey17, HyPar19,  Tensorrt}. 

When the CNN inference is executed on an embedded CPUs-GPUs MPSoC, the CNN computational workload is distributed among the heterogeneous MPSoC processors: embedded CPUs and GPUs. The CPUs are more suitable for handling task-level parallelism, compared to GPUs, whereas  GPUs are more suitable for handling data-level parallelism, compared to CPUs~\cite{Singh2017}. Thus, for efficient execution of the CNN inference on an embedded MPSoC, the task-level parallelism should be handled by the CPUs, available in an embedded MPSoC, i.e.,  different  CNN layers should, if possible, be executed on different CPUs, and the overall CNN computational workload should be balanced among the CPUs~\cite{PerfBottleneck13}. 
Additionally, the data-level parallelism, available within CNN layers, should be handled by embedded GPUs, i.e., the embedded CPUs should  offload data-parallel computations within the CNN layers onto the embedded GPUs, thereby accelerating the computations within CNN layers for further improvement of the CNN inference throughput, already achieved by efficient task-level parallelism exploitation. Thus, efficient execution of the CNN inference on an embedded CPUs-GPUs MPSoC involves efficient  exploitation of both task-level parallelism and data-level parallelism, available in the CNN. 

On the other hand, effective utilization of task- and data-level parallelism requires proper  communication and synchronization between tasks, executed on different processors of an embedded MPSoC. In this respect, attempting to utilize an  unnecessary large amount of CNN parallelism on limited embedded MPSoC resources, results in unnecessary communication and synchronization overheads, that reduce the CNN inference throughput. Thus, to achieve high CNN inference throughput, the CNN inference, executed on an embedded MPSoC, should utilize the right amount of parallelism, which matches the computational capacity of the MPSoC.  

Based on the discussion above, we argue, that  efficient  execution of the CNN inference on a CPUs-GPUs embedded MPSoC requires:  

\begin{enumerate}
\item{efficient handling of the task-level parallelism, available in a CNN, by CPUs;}
\item{CPU workload balancing;}
\item{efficient handling of the data-level parallelism, available in a CNN, by GPUs;}
\item{efficient exploitation of task- and data-level parallelism, which matches the computational capacity of an embedded MPSoC.}
\end{enumerate}

However, the existing Deep Learning (DL) frameworks~\cite{tensorflow2015, Caffe2, DLSurvey17, HyPar19,  Tensorrt, Kang2018CGOODCG, Loc2016DeepSenseAG, DeepMon17,Wang2019HighThroughputCI, Tang2018SchedulingCG}, that enable execution of the CNN inference on embedded CPUs-GPUs MPSoCs, only partially satisfy requirements 1) to 4), mentioned above. These frameworks can be divided into two main groups. The first group includes frameworks~\cite{Wang2019HighThroughputCI} and~\cite{Tang2018SchedulingCG}, that exploit  only task-level parallelism, available in a CNN, and efficiently utilize only embedded CPUs. Thus, these frameworks satisfy requirements 1) and 2), mentioned above, and do not satisfy requirement 3). The second group includes frameworks~\cite{tensorflow2015, Caffe2, DLSurvey17, HyPar19,  Tensorrt, Kang2018CGOODCG, Loc2016DeepSenseAG, DeepMon17}, that exploit only data-level parallelism, available in a CNN, and efficiently utilize only embedded GPUs. Thus, these frameworks satisfy requirement 3), mentioned above, but do not satisfy requirements 1) and 2). Moreover, all frameworks~\cite{tensorflow2015, Caffe2, DLSurvey17, HyPar19,  Tensorrt, Kang2018CGOODCG, Loc2016DeepSenseAG, DeepMon17,Wang2019HighThroughputCI, Tang2018SchedulingCG} directly utilize the CNN computational model to execute the CNN inference on embedded CPUs-GPUs MPSoCs. The large amount of parallelism, available in a CNN model, typically  does not match the limited computational capacity of embedded CPUs-GPUs MPSoC. Thus, frameworks~\cite{tensorflow2015, Caffe2, DLSurvey17, HyPar19,  Tensorrt, Kang2018CGOODCG, Loc2016DeepSenseAG, DeepMon17,Wang2019HighThroughputCI, Tang2018SchedulingCG} do not satisfy requirement 4), mentioned above. 

Therefore, in this paper, we propose a novel methodology for efficient execution of the CNN inference on embedded CPUs-GPUs MPSoCs. Our methodology consists of three main steps. In Step 1 (Section~\ref{sec:cnntosdf}), we convert a CNN model into a functionally equivalent Synchronous  Dataflow (SDF) model~\cite{SDF}. Unlike the CNN model, the SDF model explicitly specifies task- and data-level parallelism, available in a CNN, as well as it explicitly specifies the tasks  communication and synchronization mechanisms,   suitable for efficient mapping and execution of a CNN on an  embedded MPSoC. Thus, a conversion of a CNN model into a SDF model is necessary for efficient mapping and execution of a CNN on an embedded CPUs-GPUs MPSoC. In Step 2 (Section~\ref{sec:mapping}), we propose to utilize a Genetic Algorithm~\cite{GA}, to find an efficient mapping of the SDF model, obtained on Step 1, on an  embedded CPUs-GPUs MPSoC. The mapping, obtained by the Genetic Algorithm, describes the distribution of the CNN inference  computational workload on an embedded MPSoC, that satisfies requirements 1) to 3), mentioned above. In Step 3 (Section~\ref{sec:sdftocsdf}), we use the mapping, obtained in Step 2, to convert a CNN model into a final platform-aware executable Cyclo-Static Dataflow (CSDF) application model~\cite{Bilsen96}. The CSDF model, obtained in Step 3, describes the CNN inference as an application, efficiently distributed over embedded MPSoC processors and exploiting the right amount of task- and data-level parallelism, which matches the  computational capacity of an embedded MPSoC. Thus, our methodology  satisfies all requirements 1) to 4), mentioned above, to take full advantage of the CPU and GPU resources, available in an MPSoC. Moreover, as we show by  experimental results (Section~\ref{sec:Experimental_Results}), our methodology enables high-throughput execution of the CNN inference on embedded CPUs-GPUs MPSoCs. 

\subsection*{Paper contributions}
\label{sec:contrib}

In this paper, we propose a novel methodology for  execution of the CNN  inference on  embedded CPUs-GPUs MPSoCs (Section~\ref{sec:methodology}), which takes full advantage of all CPU and GPU resources, available in an MPSoC, and ensures high-throughput CNN inference execution on CPUs-GPUs MPSoCs. The exploitation of task-level (pipeline) parallelism, available among CNN layers, together with data-level parallelism, available within CNN layers, for high-throughput execution of the CNN inference on embedded MPSoCs, is our main novel contribution. Other important novel contributions are: 1) the automated conversion of a CNN model into a SDF model, suitable for searching for an efficient mapping of a CNN onto an  embedded MPSoC (Section~\ref{sec:cnntosdf}); 2) the automated conversion of a CNN model into a  functionally equivalent platform-aware executable CSDF model, which efficiently utilizes  CPUs-GPUs embedded MPSoC computational resources (Section~\ref{sec:sdftocsdf}); 3) taking state-of-the-art CNNs from the ONNX models zoo~\cite{ONNXmodelsZoo} and mapping them on a Nvidia Jetson MPSoC~\cite{JetsonTX2}, we achieve a 20\% higher throughput, when the CNN inference is executed with our methodology, compared to the throughput of the CNN inference, executed by the best-known and state-of-the-art Tensorrt DL framework~\cite{Tensorrt} for Nvidia Jetson MPSoCs (Section~\ref{sec:Experimental_Results}). 
 